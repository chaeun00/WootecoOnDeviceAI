# [오픈 미션] On-Device AI 이미지 분류 앱 개발


## 0. 메인 미션: Image Classification

## 1. 미션 목적

- **MLOps** 분야에 대한 막연한 호기심을 '이게 될까?'라는 도전 정신으로 구체화하여, Python 없이 안드로이드 앱 환경에서 AI 모델을 배포하고 활용하는 '**On-Device AI**'를 경험한다.
- 프리코스 기간 동안 학습한 클린 코드, TDD, 객체지향 설계 원칙을 새로운 기술 스택(TensorFlow Lite)에 적용하며, 자기주도적인 문제 해결 능력과 성장의 결과를 보여준다.


## 2. 사용 기술 및 모델: MobileNetV1 (Quantized)

- **초기 모델 선정:** `mobilenet_v2` (Float) 모델을 선택했으나, TFLite Task Library로 모델을 초기화하는 과정에서 `Input tensor has type kTfLiteFloat32...`라는 `RuntimeException`이 발생함.
- **모델 변경:** TFLite Task Library가 요구하는 **메타데이터가 포함**된 **Quantized (정수형) 버전**의 `mobilenet_v1` 모델로 교체함.
- **선택 이유:** 이 모델은 모바일에 최적화되어 있으며, 메타데이터가 포함되어 있어 TFLite Task Library와 호환성 문제가 없음.


## 3. 기능 요구사항

- **[핵심] 이미지 분류:**
    - [✓] 사용자가 카메라를 실행해 사진을 촬영할 수 있다.
    - [✓] TFLite 모델이 이미지(Bitmap)를 입력받아 분류 결과(라벨, 확률)를 반환하는 핵심 로직(Model)을 TDD로 구현한다.
    - [✓] 촬영된 이미지를 분석 로직에 전달하고, 반환된 결과를 화면(View)에 표시한다.

- **[부가] 갤러리 이미지 선택:**
    - [✓] 갤러리에서 이미지를 선택하여 분류할 수 있다.

- **[부가] 사용자 피드백:**
    - [✓] 이미지 처리 중 로딩 상태를 표시한다.
    - [✓] 오류 발생 시 적절한 안내 메시지를 표시한다.


## 4. 기술 및 구현 요구사항

- **[MVC 패턴]** 애플리케이션은 Model-View-Controller 패턴을 기반으로 설계한다.
    - **Model:** TFLite 모델과의 상호작용, 데이터 처리(이미지 분석, 결과 파싱)를 담당하는 핵심 로직. UI에 의존적이지 않아야 한다.
    - **View:** XML 레이아웃 및 UI 위젯. 사용자에게 정보를 시각적으로 표시하고 입력을 받는 역할.
    - **Controller:** `Activity` / `Fragment`. View로부터의 사용자 입력을 받아 Model에 처리를 요청하고, 그 결과를 View에 반영하는 중개자 역할.

- **[TDD]** Model 계층의 핵심 로직(이미지 분류 로직, 분류 결과 파싱 로직 등)은 **테스트 코드를 먼저 작성**하고 구현한다.

- **[클린 코드]**
    - 함수는 15라인을 초과하지 않는다.
    - 함수의 depth는 2단계 이하로 유지한다.
    - Kotlin/Android 코드 컨벤션을 준수한다.
    - 값을 하드코딩하지 않고, 상수나 `enum`을 활용한다.
    - 하나의 함수는 한 가지 기능만 수행하도록 한다.

- **[예외 처리]** 모델 로드, 이미지 처리, 권한 등에서 발생 가능한 예외를 명시적으로 처리한다.

---

## 5. 추가 미션 1: Semantic Segmentation (픽셀 제거)

- 기존 'Image Classification' 미션이 조기 완료되어, 미션의 난이도를 상향시키기 위해 픽셀 단위로 객체를 분류하는 '**시맨틱 세그멘테이션**' 미션을 추가로 진행한다.


### 5.1. 사용 기술 및 모델: DeepLabV3

- **모델**: `DeepLabV3 (메타데이터 포함)`
- **기술**: TFLite Task Library - `ImageSegmenter`
- **선택 이유**: `ImageClassifier`와 달리 이미지의 모든 픽셀을 '사람', '배경' 등 클래스로 분류할 수 있는 표준 모델이며, TFLite Task Library와 호환되는 메타데이터 버전을 사용함.


### 5.2. 추가 기능 요구사항

- [✓] TFLite 모델(`DeepLabV3`)이 이미지를 입력받아, 픽셀 단위 '**마스크**'와 발견된 '클래스 목록'(예: ["person", "cat"])을 반환하는 로직을 **TDD**로 구현한다.
- [✓] 모델이 반환한 '클래스 목록'을 사용자에게 동적으로 생성하여 보여준다.
- [✓] 사용자가 특정 클래스(예: "person")을 선택하고 '**제거**' 버튼을 누르면, 원본 이미지에서 해당 클래스의 픽셀을 모두 제거하고 결과를 화면에 표시한다.

---

## 6. 추가 미션 2: Inpainting (픽셀 채우기)

- 픽셀 제거 후 공백을 채우는 'Inpainting' TFLite 모델을 리서치했으나, 현재 On-Device(모바일) 환경에서 실시간으로 실행 가능한 표준 모델을 찾는 데 한계를 발견함.
- AI 모델을 대체하여, Inpainting의 핵심 알고리즘 중 하나인 **'전통적인 컴퓨터 비전(CV) 방식(주변 픽셀 평균)'**을 **TDD로 직접 구현**하여 전체 파이프라인을 완성한다.

### 6.1. 사용 기술

- **기술**: `Bitmap` 픽셀 배열을 직접 조작하는 **Average Filter** 알고리즘.

### 6.2. 추가 기능 요구사항

- [✓] 픽셀이 제거된 비트맵을 입력받아, 투명 픽셀의 **주변 픽셀 색상 평균**으로 공백을 채우는 Inpainting 로직을 **TDD**로 구현한다.
- [✓] Inpainting된 이미지를 화면에 최종 표시한다.