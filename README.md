# [오픈 미션] On-Device AI 이미지 분류 앱 개발


## 1. 미션 목적

- **MLOps** 분야에 대한 막연한 호기심을 '이게 될까?'라는 도전 정신으로 구체화하여, Python 없이 안드로이드 앱 환경에서 AI 모델을 배포하고 활용하는 '**On-Device AI**'를 경험한다.
- 프리코스 기간 동안 학습한 클린 코드, TDD, 객체지향 설계 원칙을 새로운 기술 스택(TensorFlow Lite)에 적용하며, 자기주도적인 문제 해결 능력과 성장의 결과를 보여준다.


## 2. 사용 기술 및 모델: MobileNetV1 (Quantized)

- **초기 모델 선정:** `mobilenet_v2` (Float) 모델을 선택했으나, TFLite Task Library로 모델을 초기화하는 과정에서 `Input tensor has type kTfLiteFloat32...`라는 `RuntimeException`이 발생함.
- **모델 변경:** TFLite Task Library가 요구하는 **메타데이터가 포함**된 **Quantized (정수형) 버전**의 `mobilenet_v1` 모델로 교체함.
- **선택 이유:** 이 모델은 모바일에 최적화되어 있으며, 메타데이터가 포함되어 있어 TFLite Task Library와 호환성 문제가 없음.


## 3. 기능 요구사항

- **[핵심] 이미지 분류:**
    - [✓] 사용자가 카메라를 실행해 사진을 촬영할 수 있다.
    - [✓] TFLite 모델이 이미지(Bitmap)를 입력받아 분류 결과(라벨, 확률)를 반환하는 핵심 로직(Model)을 TDD로 구현한다.
    - [✓] 촬영된 이미지를 분석 로직에 전달하고, 반환된 결과를 화면(View)에 표시한다.

- **[부가] 갤러리 이미지 선택:**
    - [ ] 갤러리에서 이미지를 선택하여 분류할 수 있다.

- **[부가] 사용자 피드백:**
    - [✓] 이미지 처리 중 로딩 상태를 표시한다.
    - [✓] 오류 발생 시 적절한 안내 메시지를 표시한다.


## 4. 기술 및 구현 요구사항

- **[MVC 패턴]** 애플리케이션은 Model-View-Controller 패턴을 기반으로 설계한다.
    - **Model:** TFLite 모델과의 상호작용, 데이터 처리(이미지 분석, 결과 파싱)를 담당하는 핵심 로직. UI에 의존적이지 않아야 한다.
    - **View:** XML 레이아웃 및 UI 위젯. 사용자에게 정보를 시각적으로 표시하고 입력을 받는 역할.
    - **Controller:** `Activity` / `Fragment`. View로부터의 사용자 입력을 받아 Model에 처리를 요청하고, 그 결과를 View에 반영하는 중개자 역할.

- **[TDD]** Model 계층의 핵심 로직(이미지 분류 로직, 분류 결과 파싱 로직 등)은 **테스트 코드를 먼저 작성**하고 구현한다.

- **[클린 코드]**
    - 함수는 15라인을 초과하지 않는다.
    - 함수의 depth는 2단계 이하로 유지한다.
    - Kotlin/Android 코드 컨벤션을 준수한다.
    - 값을 하드코딩하지 않고, 상수나 `enum`을 활용한다.
    - 하나의 함수는 한 가지 기능만 수행하도록 한다.

- **[예외 처리]** 모델 로드, 이미지 처리, 권한 등에서 발생 가능한 예외를 명시적으로 처리한다.